{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_anomalies(normal_data, n_anomalies=None, method='gaussian'):\n",
    "    \"\"\"\n",
    "    Generate synthetic anomalies using different methods\n",
    "    \n",
    "    Parameters:\n",
    "    normal_data: numpy array or pandas DataFrame of normal data\n",
    "    n_anomalies: number of anomalies to generate (default: 10% of normal data)\n",
    "    method: 'gaussian' or 'uniform' or 'extreme'\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if it's a DataFrame\n",
    "    if isinstance(normal_data, pd.DataFrame):\n",
    "        normal_data = normal_data.to_numpy()\n",
    "        \n",
    "    if n_anomalies is None:\n",
    "        n_anomalies = len(normal_data) // 10\n",
    "    \n",
    "    mean = np.mean(normal_data, axis=0)\n",
    "    std = np.std(normal_data, axis=0)\n",
    "    min_vals = np.min(normal_data, axis=0)\n",
    "    max_vals = np.max(normal_data, axis=0)\n",
    "    \n",
    "    if method == 'gaussian':\n",
    "        # Generate anomalies from a wider Gaussian distribution\n",
    "        synthetic_anomalies = np.random.normal(\n",
    "            mean, \n",
    "            3 * std,  # 3 times the standard deviation\n",
    "            size=(n_anomalies, normal_data.shape[1])\n",
    "        )\n",
    "    elif method == 'uniform':\n",
    "        # Generate anomalies uniformly outside the normal range\n",
    "        synthetic_anomalies = np.random.uniform(\n",
    "            low=min_vals - 2 * std,\n",
    "            high=max_vals + 2 * std,\n",
    "            size=(n_anomalies, normal_data.shape[1])\n",
    "        )\n",
    "    else:  # extreme method\n",
    "        # Generate extreme value anomalies\n",
    "        multipliers = np.random.choice([-3, 3], size=(n_anomalies, normal_data.shape[1]))\n",
    "        synthetic_anomalies = mean + multipliers * std\n",
    "    \n",
    "    return synthetic_anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_synthetic_anomalies(normal_data, n_estimators=100, contamination=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate Isolation Forest using synthetic anomalies\n",
    "    \n",
    "    Parameters:\n",
    "    normal_data: numpy array or pandas DataFrame of normal data\n",
    "    contamination: expected proportion of outliers\n",
    "    random_state: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    clf: trained IsolationForest model\n",
    "    metrics: dictionary containing evaluation metrics for each anomaly generation method\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Convert to numpy array if it's a DataFrame\n",
    "    if isinstance(normal_data, pd.DataFrame):\n",
    "        normal_data = normal_data.to_numpy()\n",
    "    \n",
    "    # Generate synthetic anomalies\n",
    "    n_anomalies = int(len(normal_data) * contamination)\n",
    "    \n",
    "    metrics = {}\n",
    "    clf = IsolationForest(n_estimators=n_estimators,contamination=contamination, random_state=random_state)\n",
    "    clf.fit(normal_data)\n",
    "    \n",
    "    # Test with different types of synthetic anomalies\n",
    "    for method in ['gaussian', 'uniform', 'extreme']:\n",
    "        # Generate anomalies\n",
    "        synthetic_anomalies = generate_synthetic_anomalies(normal_data, n_anomalies, method)\n",
    "        \n",
    "        # Combine normal and anomaly data\n",
    "        test_data = np.vstack([normal_data, synthetic_anomalies])\n",
    "        \n",
    "        # Create true labels (0 for normal, 1 for anomaly)\n",
    "        true_labels = np.zeros(len(test_data))\n",
    "        true_labels[len(normal_data):] = 1\n",
    "        \n",
    "        # Get predictions (convert from {1: normal, -1: anomaly} to {0: normal, 1: anomaly})\n",
    "        predictions = (clf.predict(test_data) == -1).astype(int)\n",
    "        metrics[method] = {\n",
    "            'accuracy': accuracy_score(true_labels, predictions),\n",
    "            'precision': precision_score(true_labels, predictions, zero_division=0),\n",
    "            'recall': recall_score(true_labels, predictions, zero_division=0),\n",
    "            'f1': f1_score(true_labels, predictions, zero_division=0)\n",
    "        }\n",
    "        \n",
    "    return clf, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_transactions():\n",
    "    url = \"http://localhost:3000/api/past_transactions\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json() \n",
    "    else:\n",
    "        print(\"Error fetching data from backend\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_transactions()\n",
    "df = pd.DataFrame(data)\n",
    "df['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d') if x != \"NaN-NaN-NaN\" else None)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['time_of_month'] = df['date'].dt.day\n",
    "df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# df['Type'] = df['Type'].map({\"Debit\": 0, \"Credit\": 1})\n",
    "\n",
    "# Normalize 'amount'\n",
    "scaler = StandardScaler()\n",
    "df['scaled_amount'] = scaler.fit_transform(df[['amount']])\n",
    "\n",
    "categories=df[\"category\"].unique()\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Reshape the category column into a 2D array as required by the encoder\n",
    "category_reshaped = df['category'].values.reshape(-1, 1)\n",
    "\n",
    "category_encoded = encoder.fit_transform(category_reshaped)\n",
    "\n",
    "encoded_df = pd.DataFrame(category_encoded, columns=encoder.categories_[0])\n",
    "\n",
    "df = pd.concat([df, encoded_df], axis=1).drop(columns=['category'])\n",
    "\n",
    "# Select features for training\n",
    "features = ['scaled_amount', 'hour', 'day_of_week', 'time_of_month', 'is_weekend']\n",
    "features.extend(categories)\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators: 30 and contamination of 0.05\n",
      "gaussian\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "\n",
      "For n_estimators: 30 and contamination of 0.07\n",
      "gaussian\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "\n",
      "For n_estimators: 30 and contamination of 0.1\n",
      "gaussian\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "\n",
      "For n_estimators: 50 and contamination of 0.05\n",
      "gaussian\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "\n",
      "For n_estimators: 50 and contamination of 0.07\n",
      "gaussian\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "\n",
      "For n_estimators: 50 and contamination of 0.1\n",
      "gaussian\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "\n",
      "For n_estimators: 100 and contamination of 0.05\n",
      "gaussian\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "\n",
      "For n_estimators: 100 and contamination of 0.07\n",
      "gaussian\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "\n",
      "For n_estimators: 100 and contamination of 0.1\n",
      "gaussian\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "\n",
      "For n_estimators: 150 and contamination of 0.05\n",
      "gaussian\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "\n",
      "For n_estimators: 150 and contamination of 0.07\n",
      "gaussian\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "\n",
      "For n_estimators: 150 and contamination of 0.1\n",
      "gaussian\n",
      "accuracy: 0.9102564102564102\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9102564102564102\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9102564102564102\n",
      "\n",
      "\n",
      "For n_estimators: 200 and contamination of 0.05\n",
      "gaussian\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "\n",
      "For n_estimators: 200 and contamination of 0.07\n",
      "gaussian\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "\n",
      "For n_estimators: 200 and contamination of 0.1\n",
      "gaussian\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "\n",
      "For n_estimators: 250 and contamination of 0.05\n",
      "gaussian\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9463087248322147\n",
      "\n",
      "\n",
      "For n_estimators: 250 and contamination of 0.07\n",
      "gaussian\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9337748344370861\n",
      "\n",
      "\n",
      "For n_estimators: 250 and contamination of 0.1\n",
      "gaussian\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "uniform\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "extreme\n",
      "accuracy: 0.9038461538461539\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_estimators_list=[30,50,100,150,200,250]\n",
    "contamination_list=[0.05,0.07,0.1]\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    for contamination in contamination_list:\n",
    "        print(f'For n_estimators: {n_estimators} and contamination of {contamination}')\n",
    "        clf,metrics=evaluate_with_synthetic_anomalies(X,n_estimators,contamination)\n",
    "        for key in metrics.keys():\n",
    "            print(key)\n",
    "            print(f'accuracy: {metrics[key][\"accuracy\"]}')\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing n_estimators as 100 and contamination as 0.05 <br>\n",
    "**Accuracy**: 94.63%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
